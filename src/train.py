# -*- coding: utf-8 -*-
"""enterpret.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SSRB-1ikRESsOYsVvpSpSo2UP6kKGAmR

# Structure

Mail us the link to the google drive when you are done.

All artifacts have to be in this format in a folder in google drive

- absa/src - contains all the source needed for model inferencing
- absa/notebooks - experimental notebooks
- absa/models - contains all models
- absa/main.py - API code
- absa/src/evaluation.py - should load the model and test file, generate results and save the result to `absa/data/results/test.csv` with columns `text, aspect and label`
- training_methodology - doc about your training approach
- deployment_pipeline doc - doc about your deployment pipeline in AWS for live inferencing (not a batch process)
    - You can choose any approach to deploy
    - API should be able to scale as needed
    - Load profile in live inferencing is bursty in nature i.e can have spikes of load

# Prerequisites
"""

import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score

from tensorflow.keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.layers import Dense, Activation
from keras.models import Sequential
from keras.models import load_model

import tensorflow as tf

import warnings
import pandas as pd
import numpy as np
import json
import re
import pickle
import string
import sys
import getopt

"""# Process"""

# Other

warnings.filterwarnings('ignore')

# Keras


def plot_acc_loss(history):
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()


def remove_punc(aspect):
    punc = string.punctuation
    temp = ""
    for i in aspect:
        if i not in punc:
            temp += i
    return temp


def main(argv):
    model = 'all'
    outputfile = ''
    EPOCHS = 50
    try:
        opts, args = getopt.getopt(
            argv, "h:m:o:e:", ["model=", "ofile=", "epochs="])
    except getopt.GetoptError:
        print('train.py -m <model name[dl(deep learning), lr(logistic regression[default]), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <output model name beginning> -e <epochs[default=50]>')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            print('train.py -m <model name[dl(deep learning), lr(logistic regression[default]), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <output model name beginning> -e <epochs[default=50]>')
            sys.exit()
        elif opt in ("-m", "--model"):
            model = arg
            if model not in ("all", "dl", "lr", "knn", "dt", "svc"):
                print('train.py -m <model name[dl(deep learning), lr(logistic regression[default]), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <output model name beginning> -e <epochs[default=50]>')
                sys.exit(2)
        elif opt in ("-o", "--ofile"):
            outputfile = arg
        elif opt in ("-e", "--epochs"):
            EPOCHS = arg
            if type(EPOCHS) != 'int' and EPOCHS < 1:
              print('Invalid number of epochs')
              sys.exit(2)
    model_loc = "./models/"

    df = pd.read_csv("./data/train.csv")

    vocab = []
    for i in df.iloc[:, 0]:
        vocab.append(i)
    vocab = ' '.join(vocab).split()
    vocab_size_text = len(set(vocab))

    vocab_aspect = []
    for i in df.iloc[:, 1]:
        vocab_aspect.append(i)
    vocab_aspect = ' '.join(vocab_aspect).split()
    vocab_size_aspect = len(set(vocab_aspect))

    column_count = vocab_size_text+vocab_size_aspect

    tokenizer = Tokenizer(num_words=vocab_size_text)
    tokenizer.fit_on_texts(df.text)
    df_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(df.text))

    tokenizer2 = Tokenizer(num_words=vocab_size_aspect)
    tokenizer2.fit_on_texts(df.aspect)
    df_tokenized2 = pd.DataFrame(tokenizer2.texts_to_matrix(df.aspect))

    df_tokenized3 = pd.concat([df_tokenized, df_tokenized2], axis=1)

    label_encoder = LabelEncoder()
    label_encoder.fit(df.label)
    integer_category = label_encoder.transform(df.label)
    encoded_y = to_categorical(integer_category)
    if model == 'all':
      for models in ("dl", "lr", "knn", "dt", "svc"):
        text_tok = model_loc+outputfile+'text_tokenizer_'+models
        with open(text_tok, 'wb') as handle:
          pickle.dump(tokenizer, handle)
          print("Text tokenizer saved to ", text_tok)

        aspect_tok = model_loc+outputfile+'aspect_tokenizer_'+models
        with open(aspect_tok, 'wb') as handle:
          pickle.dump(tokenizer2, handle)
          print("Aspect tokenizer saved to ", aspect_tok)

        label_en = model_loc+outputfile+'label_encoder_'+models
        with open(label_en, 'wb') as handle:
          pickle.dump(label_encoder, handle)
          print("Label Encoder saved to ", label_en)

    text_tok = model_loc+outputfile+'text_tokenizer_'+model
    with open(text_tok, 'wb') as handle:
      pickle.dump(tokenizer, handle)
      print("Text tokenizer saved to ", text_tok)

    aspect_tok = model_loc+outputfile+'aspect_tokenizer_'+model
    with open(aspect_tok, 'wb') as handle:
      pickle.dump(tokenizer2, handle)
      print("Aspect tokenizer saved to ", aspect_tok)

    label_en = model_loc+outputfile+'label_encoder_'+model
    with open(label_en, 'wb') as handle:
      pickle.dump(label_encoder, handle)
      print("Label Encoder saved to ", label_en)

    # # loading
    # with open('tokenizer.pickle', 'rb') as handle:
    #   tokenizer = pickle.load(handle)

    # Debug
    # print(df.head(5))
    # sys.exit()

    """# Deep Learning Model"""
    if model == 'all' or model == 'dl':
        print("Deep Learning -", model)
        absa_model = Sequential()
        absa_model.add(Dense(512, input_shape=(
            column_count,), activation='relu'))
        absa_model.add((Dense(256, activation='relu')))
        absa_model.add((Dense(128, activation='relu')))
        absa_model.add(Dense(3, activation='softmax'))
        # compile model
        absa_model.compile(loss='categorical_crossentropy',
                           optimizer='Adam', metrics=['accuracy'])

        history = absa_model.fit(
            df_tokenized3, encoded_y, epochs=EPOCHS, verbose=1, validation_split=0.1)
        model_name = model_loc+outputfile+'dl'
        absa_model.save(model_name)
        print("Model saved to ", model_name)

        # history.history['accuracy'].index(max(history.history['accuracy']))

        # history.history['val_accuracy'].index(max(history.history['val_accuracy']))

        # history.history['loss'].index(min(history.history['loss']))

        # history.history['val_loss'].index(min(history.history['val_loss']))

        # print(history.history.keys())

        # Visualize training history

    if model == 'all' or model == 'dl':
        plot_acc_loss(history)

    """# Load for Training"""

    X = df_tokenized3.iloc[:, :].values
    y = df.iloc[:, 2].values

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)

    """# Linear Regression"""
    if model == 'all' or model == 'lr':
        print("Linear Regression -", model)
        absa_model = LogisticRegression()
        absa_model.fit(X_train, y_train)
        pkl_filename = model_loc+outputfile+'lr.pkl'
        with open(pkl_filename, 'wb') as file:
            pickle.dump(absa_model, file)
        print("Model saved to ", pkl_filename)
        print("Model accuracy on validation: ", absa_model.score(X_valid, y_valid))
        print(classification_report(y_valid, absa_model.predict(X_valid).tolist(), target_names=['0', '1', '2']))

    """# KNN"""
    if model == 'all' or model == 'knn':
        print("KNN -", model)
        absa_model = KNeighborsClassifier()
        absa_model.fit(X_train, y_train)
        pkl_filename = model_loc+outputfile+'knn.pkl'
        with open(pkl_filename, 'wb') as file:
            pickle.dump(absa_model, file)
        print("Model saved to ", pkl_filename)
        print("Model accuracy on validation: ", absa_model.score(X_valid, y_valid))
        print(classification_report(y_valid, absa_model.predict(X_valid).tolist(), target_names=['0', '1', '2']))


    """# Decision Tree"""
    if model == 'all' or model == 'dt':
        print("Decision Tree -", model)
        absa_model = DecisionTreeClassifier()
        absa_model.fit(X_train, y_train)
        pkl_filename = model_loc+outputfile+'lr.dt'
        with open(pkl_filename, 'wb') as file:
            pickle.dump(absa_model, file)
        print("Model saved to ", pkl_filename)
        print("Model accuracy on validation: ", absa_model.score(X_valid, y_valid))
        print(classification_report(y_valid, absa_model.predict(X_valid).tolist(), target_names=['0', '1', '2']))

    """# SVC"""
    if model == 'all' or model == 'svc':
        print("SVC -", model)
        absa_model = SVC(kernel='rbf')
        absa_model.fit(X_train, y_train.reshape(y_train.shape[0], ))
        pkl_filename = model_loc+outputfile+'svc.pkl'
        with open(pkl_filename, 'wb') as file:
            pickle.dump(absa_model, file)
        print("Model saved to ", pkl_filename)
        print("Model accuracy on validation: ", absa_model.score(X_valid, y_valid))
        print(classification_report(y_valid, absa_model.predict(X_valid).tolist(), target_names=['0', '1', '2']))



if __name__ == "__main__":
    main(sys.argv[1:])
