# -*- coding: utf-8 -*-
"""enterpret.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SSRB-1ikRESsOYsVvpSpSo2UP6kKGAmR

# Structure

Mail us the link to the google drive when you are done.

All artifacts have to be in this format in a folder in google drive

- absa/src - contains all the source needed for model inferencing
- absa/notebooks - experimental notebooks
- absa/models - contains all models
- absa/main.py - API code
- absa/src/evaluation.py - should load the model and test file, generate results and save the result to `absa/data/results/test.csv` with columns `text, aspect and label`
- training_methodology - doc about your training approach
- deployment_pipeline doc - doc about your deployment pipeline in AWS for live inferencing (not a batch process)
    - You can choose any approach to deploy
    - API should be able to scale as needed
    - Load profile in live inferencing is bursty in nature i.e can have spikes of load

# Prerequisites
"""

import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.layers import Dense, Activation
from keras.models import Sequential
from keras.models import load_model
import tensorflow as tf
import warnings
import pandas as pd
import numpy as np
import json
import re
import pickle
import string
import sys
import getopt

# Other

warnings.filterwarnings('ignore')

# Keras


def remove_punc(aspect):
    punc = string.punctuation
    temp = ""
    for i in aspect:
        if i not in punc:
            temp += i
    return temp


def main(argv):
    model = 'all'
    outputfile = ''
    csvfile = 'test' 
    try:
        opts, args = getopt.getopt(
            argv, "h:m:o:c:", ["model=", "ofile=", "csvfile="])
    except getopt.GetoptError:
        print('train.py -m <model name[all(all models), dl(deep learning), lr(logistic regression), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <model name beginning> -c <output csv file name>')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            print('train.py -m <model name[all(all models), dl(deep learning), lr(logistic regression), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <model name beginning> -c <output csv file name>')
            sys.exit()
        elif opt in ("-m", "--model"):
            model = arg
            if model not in ("all", "dl", "lr", "knn", "dt", "svc"):
                print('train.py -m <model name[all(all models), dl(deep learning), lr(logistic regression), knn(k nearest neighbour), dt(decision tree), svc(support vector classifier)]> -o <model name beginning> -c <output csv file name>')
                sys.exit(2)
        elif opt in ("-o", "--ofile"):
            outputfile = arg
        elif opt in ("-c", "--csvfile"):
            csvfile = arg
    model_loc = "./models/"

    test = pd.read_csv('./data/test.csv')

    test_text = test.iloc[:, 0].values.tolist()

    
    text_tok = model_loc+outputfile+'text_tokenizer_'+model
    with open(text_tok, 'rb') as handle:
      tokenizer = pickle.load(handle)
      print("Text tokenizer loaded from ", text_tok)

    aspect_tok = model_loc+outputfile+'aspect_tokenizer_'+model
    with open(aspect_tok, 'rb') as handle:
      tokenizer2 = pickle.load(handle)
      print("Aspect tokenizer loaded from ", aspect_tok)

    label_en = model_loc+outputfile+'label_encoder_'+model
    with open(label_en, 'rb') as handle:
      label_encoder = pickle.load(handle)
      print("Label Encoder loaded from ", label_en)

    # Aspect preprocessing
    test_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(test.text))
    test_tokenized2 = pd.DataFrame(tokenizer2.texts_to_matrix(test.aspect))
    test_tokenized3 = pd.concat([test_tokenized, test_tokenized2], axis=1)


    """# Deep Learning Model"""
    if model == 'all' or model == 'dl':
        print("Deep Learning -", model)
        model_name = model_loc+outputfile+'dl'
        absa_model = load_model(model_name)

        print("Model loaded from ", model_name)

        test_sentiment = label_encoder.inverse_transform(np.argmax(absa_model.predict(test_tokenized3), axis=-1))
        test['label'] = test_sentiment
        save_name = csvfile
        if model == 'all':
            save_name = csvfile + 'dl'
        test.to_csv('./data/results/'+save_name+'.csv', index=False)
        print("Result saved to ", str('./data/results/'+save_name+'.csv'))

    """# Linear Regression"""
    if model == 'all' or model == 'lr':
        print("Linear Regression -", model)
        pkl_filename = model_loc+outputfile+'lr.pkl'
        with open(pkl_filename, 'rb') as file:
            absa_model = pickle.load(file)
        print("Model loaded from ", pkl_filename)

        test_sentiment = absa_model.predict(test_tokenized3).tolist()
        test['label'] = test_sentiment
        save_name = csvfile
        if model == 'all':
            save_name = csvfile + 'lr'
        test.to_csv('./data/results/'+save_name+'.csv', index=False)
        print("Result saved to ", str('./data/results/'+save_name+'.csv'))

    """# KNN"""
    if model == 'all' or model == 'knn':
        print("KNN -", model)
        pkl_filename = model_loc+outputfile+'knn.pkl'
        with open(pkl_filename, 'rb') as file:
            absa_model = pickle.load(file)
        print("Model loaded from ", pkl_filename)
        
        test_sentiment = absa_model.predict(test_tokenized3).tolist()
        test['label'] = test_sentiment
        save_name = csvfile
        if model == 'all':
            save_name = csvfile + 'knn'
        test.to_csv('./data/results/'+save_name+'.csv', index=False)
        print("Result saved to ", str('./data/results/'+save_name+'.csv'))

    """# Decision Tree"""
    if model == 'all' or model == 'dt':
        print("Decision Tree -", model)
        pkl_filename = model_loc+outputfile+'lr.dt'
        with open(pkl_filename, 'rb') as file:
            absa_model = pickle.load(file)
        print("Model loaded from ", pkl_filename)
        
        test_sentiment = absa_model.predict(test_tokenized3).tolist()
        test['label'] = test_sentiment
        save_name = csvfile
        if model == 'all':
            save_name = csvfile + 'dt'
        test.to_csv('./data/results/'+save_name+'.csv', index=False)
        print("Result saved to ", str('./data/results/'+save_name+'.csv'))

    """# SVC"""
    if model == 'all' or model == 'svc':
        print("SVC -", model)
        pkl_filename = model_loc+outputfile+'svc.pkl'
        with open(pkl_filename, 'rb') as file:
            absa_model = pickle.load(file)
        print("Model loaded from ", pkl_filename)
        
        test_sentiment = absa_model.predict(test_tokenized3).tolist()
        test['label'] = test_sentiment
        save_name = csvfile
        if model == 'all':
            save_name = csvfile + 'svc'
        test.to_csv('./data/results/'+save_name+'.csv', index=False)
        print("Result saved to ", str('./data/results/'+save_name+'.csv'))
        

if __name__ == "__main__":
    main(sys.argv[1:])
